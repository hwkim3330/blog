{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwkim3330/blog/blob/main/%EC%A0%9C1%ED%9A%8C_%ED%80%80%ED%85%80AI_%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 0: 라이브러리 설치 및 임포트\n",
        "# ==============================================================================\n",
        "# 필수 라이브러리를 설치합니다.\n",
        "!pip install pennylane --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: 하이퍼파라미터 및 환경 설정\n",
        "# ==============================================================================\n",
        "# 이곳의 값들을 조정하며 추가 실험을 할 수 있습니다.\n",
        "class HParams:\n",
        "    # 데이터 및 학습 관련\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 15  # 충분한 학습을 위해 에폭 증가\n",
        "    LEARNING_RATE = 1e-3\n",
        "    VALIDATION_SPLIT = 0.1 # 학습 데이터 중 10%를 검증에 사용\n",
        "\n",
        "    # 양자 회로(QNN) 관련\n",
        "    N_QUBITS = 4 # 양자 회로에 사용할 큐빗 수 (최대 8)\n",
        "    N_LAYERS = 4 # 양자 회로의 깊이 (StronglyEntanglingLayers 반복 횟수)\n",
        "    Q_DEVICE = \"default.qubit\" # PennyLane 시뮬레이터\n",
        "\n",
        "    # 실행 환경\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    SEED = 42\n",
        "\n",
        "# 재현성을 위한 시드 고정\n",
        "torch.manual_seed(HParams.SEED)\n",
        "np.random.seed(HParams.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(HParams.SEED)\n",
        "\n",
        "print(f\"Hyperparameters loaded. Using device: {HParams.DEVICE}\")\n",
        "print(f\"Quantum circuit config: {HParams.N_QUBITS} qubits, {HParams.N_LAYERS} layers.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: 데이터셋 준비 (데이터 증강 포함)\n",
        "# ==============================================================================\n",
        "# 학습 데이터에는 회전, 뒤집기 등 데이터 증강을 적용하여 모델의 일반화 성능 향상\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 테스트 데이터는 증강을 적용하지 않음\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# FashionMNIST 데이터셋 다운로드\n",
        "full_train_ds = torchvision.datasets.FashionMNIST(\"./\", train=True, download=True, transform=train_transform)\n",
        "test_ds = torchvision.datasets.FashionMNIST(\"./\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "# T-shirt(0)와 Shirt(6) 라벨만 필터링\n",
        "train_mask = (full_train_ds.targets == 0) | (full_train_ds.targets == 6)\n",
        "train_indices = torch.where(train_mask)[0]\n",
        "full_train_ds.targets[full_train_ds.targets == 6] = 1 # 라벨 6을 1로 변환\n",
        "binary_train_ds = Subset(full_train_ds, train_indices)\n",
        "\n",
        "# 학습/검증 데이터 분리\n",
        "num_train = int((1 - HParams.VALIDATION_SPLIT) * len(binary_train_ds))\n",
        "num_val = len(binary_train_ds) - num_train\n",
        "train_ds, val_ds = random_split(binary_train_ds, [num_train, num_val])\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_ds, batch_size=HParams.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=HParams.BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=HParams.BATCH_SIZE, shuffle=False) # 테스트 시에도 배치 처리로 속도 향상\n",
        "\n",
        "print(f\"Data loaded. Train: {len(train_ds)}, Validation: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: 하이브리드 모델 설계 (CNN + QNN)\n",
        "# ==============================================================================\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "class HybridClassifier(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "\n",
        "        # 1. 클래식 파트 (CNN) - 특징 추출기\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # 클래식 파트의 최종 출력을 양자 회로의 입력 수에 맞게 조정\n",
        "        self.fc_classic = nn.Sequential(\n",
        "            nn.Linear(16 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.n_qubits) # 최종 출력을 n_qubits개로 맞춤\n",
        "        )\n",
        "\n",
        "        # 2. 양자 파트 (QNN)\n",
        "        self.q_device = qml.device(HParams.Q_DEVICE, wires=self.n_qubits)\n",
        "        # 학습 가능한 양자 회로의 가중치\n",
        "        q_weight_shape = (n_layers, self.n_qubits, 3)\n",
        "        self.q_weights = nn.Parameter(torch.rand(q_weight_shape) * 2 * torch.pi)\n",
        "\n",
        "        # qml.qnode 데코레이터를 사용하여 파이토치와 호환되는 양자 회로 정의\n",
        "        @qml.qnode(self.q_device, interface=\"torch\", diff_method=\"backprop\")\n",
        "        def quantum_circuit(features, weights):\n",
        "            # 데이터 인코딩: 클래식 데이터를 양자 상태로 변환\n",
        "            qml.AngleEmbedding(features, wires=range(self.n_qubits))\n",
        "            # 학습 가능한 변분 회로(Variational Circuit)\n",
        "            qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
        "            # 측정: 0번 큐빗의 Pauli-Z 연산자 기댓값을 반환\n",
        "            return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "        self.qnn = quantum_circuit\n",
        "\n",
        "        # 3. 최종 분류기\n",
        "        # 양자 회로의 출력(-1 ~ 1)을 클래스 로짓(logits)으로 변환\n",
        "        self.fc_classifier = nn.Linear(1, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape: [batch, 1, 28, 28]\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = self.fc_classic(x) # shape: [batch, n_qubits]\n",
        "\n",
        "        # 양자 회로를 배치 단위로 실행\n",
        "        q_out_list = []\n",
        "        for features in x:\n",
        "            q_out = self.qnn(features, self.q_weights)\n",
        "            q_out_list.append(q_out)\n",
        "\n",
        "        q_out_batch = torch.stack(q_out_list).view(-1, 1) # shape: [batch, 1]\n",
        "\n",
        "        # 최종 분류\n",
        "        logits = self.fc_classifier(q_out_batch) # shape: [batch, 2]\n",
        "        return F.log_softmax(logits, dim=1)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: 대회 규격 검증\n",
        "# ==============================================================================\n",
        "model = HybridClassifier(HParams.N_QUBITS, HParams.N_LAYERS).to(HParams.DEVICE)\n",
        "\n",
        "# 더미 입력을 만들어 회로 제약 검증\n",
        "dummy_features = torch.randn(HParams.N_QUBITS, dtype=torch.float64)\n",
        "dummy_weights = torch.randn((HParams.N_LAYERS, HParams.N_QUBITS, 3), dtype=torch.float64)\n",
        "specs = qml.specs(model.qnn)(dummy_features, dummy_weights)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"─\" * 30)\n",
        "print(\"대회 규격 자동 검증\")\n",
        "print(\"─\" * 30)\n",
        "try:\n",
        "    assert specs[\"num_tape_wires\"] <= 8, f\"❌ 큐빗 수 초과 ({specs['num_tape_wires']} > 8)\"\n",
        "    print(f\"✅ 큐빗 수: {specs['num_tape_wires']} (제한: 8)\")\n",
        "\n",
        "    assert specs['resources'].depth <= 30, f\"❌ 회로 깊이 초과 ({specs['resources'].depth} > 30)\"\n",
        "    print(f\"✅ 회로 깊이: {specs['resources'].depth} (제한: 30)\")\n",
        "\n",
        "    num_quantum_params = model.q_weights.numel()\n",
        "    assert num_quantum_params <= 60, f\"❌ 학습 퀀텀 파라미터 수 초과 ({num_quantum_params} > 60)\"\n",
        "    print(f\"✅ 퀀텀 파라미터 수: {num_quantum_params} (제한: 60)\")\n",
        "\n",
        "    assert total_params <= 50000, f\"❌ 학습 전체 파라미터 수 초과 ({total_params} > 50000)\"\n",
        "    print(f\"✅ 전체 파라미터 수: {total_params} (제한: 50000)\")\n",
        "\n",
        "    print(\"\\n🎉 모든 규격 통과! 학습을 시작합니다.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n🚨 규격 위반! {e}\")\n",
        "print(\"─\" * 30)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: 모델 학습 및 검증\n",
        "# ==============================================================================\n",
        "optimizer = AdamW(model.parameters(), lr=HParams.LEARNING_RATE)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * HParams.EPOCHS)\n",
        "loss_func = nn.NLLLoss()\n",
        "\n",
        "best_val_acc = 0.0\n",
        "BEST_MODEL_PATH = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(HParams.EPOCHS):\n",
        "    # --- 학습 ---\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{HParams.EPOCHS} [Train]\", leave=False)\n",
        "    for data, target in train_bar:\n",
        "        data, target = data.to(HParams.DEVICE), target.to(HParams.DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_func(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # --- 검증 ---\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{HParams.EPOCHS} [Val]\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_bar:\n",
        "            data, target = data.to(HParams.DEVICE), target.to(HParams.DEVICE)\n",
        "            output = model(data)\n",
        "            loss = loss_func(output, target)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{HParams.EPOCHS} | \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # 최고 성능 모델 저장\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"✨ New best model saved with accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: 최종 추론 및 제출 파일 생성\n",
        "# ==============================================================================\n",
        "print(\"\\nLoading best model for final inference...\")\n",
        "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in tqdm(test_loader, desc=\"Final Inference\"):\n",
        "        data = data.to(HParams.DEVICE)\n",
        "        logits = model(data)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_targets.append(target) # target은 이미 CPU에 있음\n",
        "\n",
        "y_pred = torch.cat(all_preds).numpy().astype(int)\n",
        "y_true = torch.cat(all_targets).numpy().astype(int)\n",
        "\n",
        "# --- 평가 및 결과 저장 ---\n",
        "# 0(T-shirt/top)과 6(Shirt) 라벨만 평가\n",
        "test_mask = (y_true == 0) | (y_true == 6)\n",
        "target_samples_count = test_mask.sum()\n",
        "\n",
        "# 모델의 예측 결과(0, 1)를 원래 라벨(0, 6)으로 되돌림\n",
        "y_pred_mapped = np.where(y_pred == 1, 6, y_pred)\n",
        "\n",
        "# 0과 6 라벨에 대한 정확도 계산\n",
        "final_acc = (y_pred_mapped[test_mask] == y_true[test_mask]).mean()\n",
        "print(f\"\\nFinal Accuracy on Target Labels (0/6): {final_acc:.4f}\")\n",
        "\n",
        "# --- 제출 파일 생성 ---\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "y_pred_filename = f\"y_pred_{now}.csv\"\n",
        "np.savetxt(y_pred_filename, y_pred_mapped, fmt=\"%d\", delimiter=',')\n",
        "print(f\"Submission file '{y_pred_filename}' created.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 7: 결과 파일 다운로드\n",
        "# ==============================================================================\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(y_pred_filename)\n",
        "    print(\"File download initiated.\")\n",
        "except ImportError:\n",
        "    print(\"\\nNot in Google Colab. Please download the file manually:\")\n",
        "    print(f\"-> {os.path.join(os.getcwd(), y_pred_filename)}\")"
      ],
      "metadata": {
        "id": "0m2zEp6wk_vm"
      },
      "id": "0m2zEp6wk_vm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}