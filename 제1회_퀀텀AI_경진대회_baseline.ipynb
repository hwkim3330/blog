{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwkim3330/blog/blob/main/%EC%A0%9C1%ED%9A%8C_%ED%80%80%ED%85%80AI_%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
        "# ==============================================================================\n",
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
        "!pip install pennylane --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° í™˜ê²½ ì„¤ì •\n",
        "# ==============================================================================\n",
        "# ì´ê³³ì˜ ê°’ë“¤ì„ ì¡°ì •í•˜ë©° ì¶”ê°€ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "class HParams:\n",
        "    # ë°ì´í„° ë° í•™ìŠµ ê´€ë ¨\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 15  # ì¶©ë¶„í•œ í•™ìŠµì„ ìœ„í•´ ì—í­ ì¦ê°€\n",
        "    LEARNING_RATE = 1e-3\n",
        "    VALIDATION_SPLIT = 0.1 # í•™ìŠµ ë°ì´í„° ì¤‘ 10%ë¥¼ ê²€ì¦ì— ì‚¬ìš©\n",
        "\n",
        "    # ì–‘ì íšŒë¡œ(QNN) ê´€ë ¨\n",
        "    N_QUBITS = 4 # ì–‘ì íšŒë¡œì— ì‚¬ìš©í•  íë¹— ìˆ˜ (ìµœëŒ€ 8)\n",
        "    N_LAYERS = 4 # ì–‘ì íšŒë¡œì˜ ê¹Šì´ (StronglyEntanglingLayers ë°˜ë³µ íšŸìˆ˜)\n",
        "    Q_DEVICE = \"default.qubit\" # PennyLane ì‹œë®¬ë ˆì´í„°\n",
        "\n",
        "    # ì‹¤í–‰ í™˜ê²½\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    SEED = 42\n",
        "\n",
        "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
        "torch.manual_seed(HParams.SEED)\n",
        "np.random.seed(HParams.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(HParams.SEED)\n",
        "\n",
        "print(f\"Hyperparameters loaded. Using device: {HParams.DEVICE}\")\n",
        "print(f\"Quantum circuit config: {HParams.N_QUBITS} qubits, {HParams.N_LAYERS} layers.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: ë°ì´í„°ì…‹ ì¤€ë¹„ (ë°ì´í„° ì¦ê°• í¬í•¨)\n",
        "# ==============================================================================\n",
        "# í•™ìŠµ ë°ì´í„°ì—ëŠ” íšŒì „, ë’¤ì§‘ê¸° ë“± ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì¦ê°•ì„ ì ìš©í•˜ì§€ ì•ŠìŒ\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# FashionMNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
        "full_train_ds = torchvision.datasets.FashionMNIST(\"./\", train=True, download=True, transform=train_transform)\n",
        "test_ds = torchvision.datasets.FashionMNIST(\"./\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "# T-shirt(0)ì™€ Shirt(6) ë¼ë²¨ë§Œ í•„í„°ë§\n",
        "train_mask = (full_train_ds.targets == 0) | (full_train_ds.targets == 6)\n",
        "train_indices = torch.where(train_mask)[0]\n",
        "full_train_ds.targets[full_train_ds.targets == 6] = 1 # ë¼ë²¨ 6ì„ 1ë¡œ ë³€í™˜\n",
        "binary_train_ds = Subset(full_train_ds, train_indices)\n",
        "\n",
        "# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
        "num_train = int((1 - HParams.VALIDATION_SPLIT) * len(binary_train_ds))\n",
        "num_val = len(binary_train_ds) - num_train\n",
        "train_ds, val_ds = random_split(binary_train_ds, [num_train, num_val])\n",
        "\n",
        "# ë°ì´í„°ë¡œë” ìƒì„±\n",
        "train_loader = DataLoader(train_ds, batch_size=HParams.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=HParams.BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=HParams.BATCH_SIZE, shuffle=False) # í…ŒìŠ¤íŠ¸ ì‹œì—ë„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ\n",
        "\n",
        "print(f\"Data loaded. Train: {len(train_ds)}, Validation: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„¤ê³„ (CNN + QNN)\n",
        "# ==============================================================================\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "class HybridClassifier(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "\n",
        "        # 1. í´ë˜ì‹ íŒŒíŠ¸ (CNN) - íŠ¹ì§• ì¶”ì¶œê¸°\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # í´ë˜ì‹ íŒŒíŠ¸ì˜ ìµœì¢… ì¶œë ¥ì„ ì–‘ì íšŒë¡œì˜ ì…ë ¥ ìˆ˜ì— ë§ê²Œ ì¡°ì •\n",
        "        self.fc_classic = nn.Sequential(\n",
        "            nn.Linear(16 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.n_qubits) # ìµœì¢… ì¶œë ¥ì„ n_qubitsê°œë¡œ ë§ì¶¤\n",
        "        )\n",
        "\n",
        "        # 2. ì–‘ì íŒŒíŠ¸ (QNN)\n",
        "        self.q_device = qml.device(HParams.Q_DEVICE, wires=self.n_qubits)\n",
        "        # í•™ìŠµ ê°€ëŠ¥í•œ ì–‘ì íšŒë¡œì˜ ê°€ì¤‘ì¹˜\n",
        "        q_weight_shape = (n_layers, self.n_qubits, 3)\n",
        "        self.q_weights = nn.Parameter(torch.rand(q_weight_shape) * 2 * torch.pi)\n",
        "\n",
        "        # qml.qnode ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì´í† ì¹˜ì™€ í˜¸í™˜ë˜ëŠ” ì–‘ì íšŒë¡œ ì •ì˜\n",
        "        @qml.qnode(self.q_device, interface=\"torch\", diff_method=\"backprop\")\n",
        "        def quantum_circuit(features, weights):\n",
        "            # ë°ì´í„° ì¸ì½”ë”©: í´ë˜ì‹ ë°ì´í„°ë¥¼ ì–‘ì ìƒíƒœë¡œ ë³€í™˜\n",
        "            qml.AngleEmbedding(features, wires=range(self.n_qubits))\n",
        "            # í•™ìŠµ ê°€ëŠ¥í•œ ë³€ë¶„ íšŒë¡œ(Variational Circuit)\n",
        "            qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
        "            # ì¸¡ì •: 0ë²ˆ íë¹—ì˜ Pauli-Z ì—°ì‚°ì ê¸°ëŒ“ê°’ì„ ë°˜í™˜\n",
        "            return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "        self.qnn = quantum_circuit\n",
        "\n",
        "        # 3. ìµœì¢… ë¶„ë¥˜ê¸°\n",
        "        # ì–‘ì íšŒë¡œì˜ ì¶œë ¥(-1 ~ 1)ì„ í´ë˜ìŠ¤ ë¡œì§“(logits)ìœ¼ë¡œ ë³€í™˜\n",
        "        self.fc_classifier = nn.Linear(1, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape: [batch, 1, 28, 28]\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = self.fc_classic(x) # shape: [batch, n_qubits]\n",
        "\n",
        "        # ì–‘ì íšŒë¡œë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹¤í–‰\n",
        "        q_out_list = []\n",
        "        for features in x:\n",
        "            q_out = self.qnn(features, self.q_weights)\n",
        "            q_out_list.append(q_out)\n",
        "\n",
        "        q_out_batch = torch.stack(q_out_list).view(-1, 1) # shape: [batch, 1]\n",
        "\n",
        "        # ìµœì¢… ë¶„ë¥˜\n",
        "        logits = self.fc_classifier(q_out_batch) # shape: [batch, 2]\n",
        "        return F.log_softmax(logits, dim=1)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: ëŒ€íšŒ ê·œê²© ê²€ì¦\n",
        "# ==============================================================================\n",
        "model = HybridClassifier(HParams.N_QUBITS, HParams.N_LAYERS).to(HParams.DEVICE)\n",
        "\n",
        "# ë”ë¯¸ ì…ë ¥ì„ ë§Œë“¤ì–´ íšŒë¡œ ì œì•½ ê²€ì¦\n",
        "dummy_features = torch.randn(HParams.N_QUBITS, dtype=torch.float64)\n",
        "dummy_weights = torch.randn((HParams.N_LAYERS, HParams.N_QUBITS, 3), dtype=torch.float64)\n",
        "specs = qml.specs(model.qnn)(dummy_features, dummy_weights)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"â”€\" * 30)\n",
        "print(\"ëŒ€íšŒ ê·œê²© ìë™ ê²€ì¦\")\n",
        "print(\"â”€\" * 30)\n",
        "try:\n",
        "    assert specs[\"num_tape_wires\"] <= 8, f\"âŒ íë¹— ìˆ˜ ì´ˆê³¼ ({specs['num_tape_wires']} > 8)\"\n",
        "    print(f\"âœ… íë¹— ìˆ˜: {specs['num_tape_wires']} (ì œí•œ: 8)\")\n",
        "\n",
        "    assert specs['resources'].depth <= 30, f\"âŒ íšŒë¡œ ê¹Šì´ ì´ˆê³¼ ({specs['resources'].depth} > 30)\"\n",
        "    print(f\"âœ… íšŒë¡œ ê¹Šì´: {specs['resources'].depth} (ì œí•œ: 30)\")\n",
        "\n",
        "    num_quantum_params = model.q_weights.numel()\n",
        "    assert num_quantum_params <= 60, f\"âŒ í•™ìŠµ í€€í…€ íŒŒë¼ë¯¸í„° ìˆ˜ ì´ˆê³¼ ({num_quantum_params} > 60)\"\n",
        "    print(f\"âœ… í€€í…€ íŒŒë¼ë¯¸í„° ìˆ˜: {num_quantum_params} (ì œí•œ: 60)\")\n",
        "\n",
        "    assert total_params <= 50000, f\"âŒ í•™ìŠµ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ì´ˆê³¼ ({total_params} > 50000)\"\n",
        "    print(f\"âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params} (ì œí•œ: 50000)\")\n",
        "\n",
        "    print(\"\\nğŸ‰ ëª¨ë“  ê·œê²© í†µê³¼! í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\nğŸš¨ ê·œê²© ìœ„ë°˜! {e}\")\n",
        "print(\"â”€\" * 30)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦\n",
        "# ==============================================================================\n",
        "optimizer = AdamW(model.parameters(), lr=HParams.LEARNING_RATE)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * HParams.EPOCHS)\n",
        "loss_func = nn.NLLLoss()\n",
        "\n",
        "best_val_acc = 0.0\n",
        "BEST_MODEL_PATH = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(HParams.EPOCHS):\n",
        "    # --- í•™ìŠµ ---\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{HParams.EPOCHS} [Train]\", leave=False)\n",
        "    for data, target in train_bar:\n",
        "        data, target = data.to(HParams.DEVICE), target.to(HParams.DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_func(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # --- ê²€ì¦ ---\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{HParams.EPOCHS} [Val]\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_bar:\n",
        "            data, target = data.to(HParams.DEVICE), target.to(HParams.DEVICE)\n",
        "            output = model(data)\n",
        "            loss = loss_func(output, target)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{HParams.EPOCHS} | \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"âœ¨ New best model saved with accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: ìµœì¢… ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "# ==============================================================================\n",
        "print(\"\\nLoading best model for final inference...\")\n",
        "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in tqdm(test_loader, desc=\"Final Inference\"):\n",
        "        data = data.to(HParams.DEVICE)\n",
        "        logits = model(data)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_targets.append(target) # targetì€ ì´ë¯¸ CPUì— ìˆìŒ\n",
        "\n",
        "y_pred = torch.cat(all_preds).numpy().astype(int)\n",
        "y_true = torch.cat(all_targets).numpy().astype(int)\n",
        "\n",
        "# --- í‰ê°€ ë° ê²°ê³¼ ì €ì¥ ---\n",
        "# 0(T-shirt/top)ê³¼ 6(Shirt) ë¼ë²¨ë§Œ í‰ê°€\n",
        "test_mask = (y_true == 0) | (y_true == 6)\n",
        "target_samples_count = test_mask.sum()\n",
        "\n",
        "# ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼(0, 1)ë¥¼ ì›ë˜ ë¼ë²¨(0, 6)ìœ¼ë¡œ ë˜ëŒë¦¼\n",
        "y_pred_mapped = np.where(y_pred == 1, 6, y_pred)\n",
        "\n",
        "# 0ê³¼ 6 ë¼ë²¨ì— ëŒ€í•œ ì •í™•ë„ ê³„ì‚°\n",
        "final_acc = (y_pred_mapped[test_mask] == y_true[test_mask]).mean()\n",
        "print(f\"\\nFinal Accuracy on Target Labels (0/6): {final_acc:.4f}\")\n",
        "\n",
        "# --- ì œì¶œ íŒŒì¼ ìƒì„± ---\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "y_pred_filename = f\"y_pred_{now}.csv\"\n",
        "np.savetxt(y_pred_filename, y_pred_mapped, fmt=\"%d\", delimiter=',')\n",
        "print(f\"Submission file '{y_pred_filename}' created.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 7: ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
        "# ==============================================================================\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(y_pred_filename)\n",
        "    print(\"File download initiated.\")\n",
        "except ImportError:\n",
        "    print(\"\\nNot in Google Colab. Please download the file manually:\")\n",
        "    print(f\"-> {os.path.join(os.getcwd(), y_pred_filename)}\")"
      ],
      "metadata": {
        "id": "0m2zEp6wk_vm"
      },
      "id": "0m2zEp6wk_vm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}